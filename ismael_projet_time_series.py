# -*- coding: utf-8 -*-
"""Ismael_projet_time_series.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zU00bAMFfnKypY6eP-YU_TmYNymkyR66
"""

#Imporatation des packages
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
from pmdarima.model_selection import train_test_split as time_train_test_split
from sklearn import metrics
from sklearn.metrics import *
from prophet import *

data=pd.read_csv("oil_price.csv")

data.info

data.dtypes

data.rename(columns={"ULSP":"Price"},inplace=True)

data["Date"]=pd.to_datetime(data["Date"])

data.head

# Test de stationnarité
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.stattools import adfuller
#Perform Dickey-Fuller test:
print ('Results of Dickey-Fuller Test:')
dftest = adfuller(data.Price, autolag='AIC')
dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
for key,value in dftest[4].items():
    dfoutput['Critical Value (%s)'%key] = value
print (dfoutput)

# Test de stationnarité
def test_adf(series, title=''):
    dfout={}
    dftest=sm.tsa.adfuller(series.dropna(), autolag='AIC', regression='ct')
    for key,val in dftest[4].items():
        dfout[f'critical value ({key})']=val
    if dftest[1]<=0.05:
        print("Strong evidence against Null Hypothesis")
        print("Reject Null Hypothesis - Data is Stationary")
        print("Data is Stationary", title)
    else:
        print("Strong evidence for  Null Hypothesis")
        print("Accept Null Hypothesis - Data is not Stationary")
        print("Data is NOT Stationary for", title)
test_adf(data.Price, 'Résultats du test adf')

# Chargement des données
data_dec = pd.read_csv('oil_price.csv', index_col='Date', parse_dates=True)
data_deco=data_dec
data_deco.index = pd.to_datetime(data_deco.index)

# Modèle additf ou multiplicatif pour la décomposition ?
import matplotlib.pyplot as plt

ts_level = data_deco.groupby(data_deco.index.year).mean()  # prix annuel comme niveau de la série
ts_var = data_deco.groupby(data_deco.index.year).var()  # variance annuelle des prix de l'ULSP

plt.scatter(ts_level, ts_var)  # nuage de points
plt.xlabel('Level')
plt.ylabel('Variance')
plt.title('Variance vs. Level Plot')
plt.show()

# Décompostion de la série
from statsmodels.tsa.seasonal import seasonal_decompose
from dateutil.parser import parse


# Multiplicative Decomposition
multiplicative_decomposition = seasonal_decompose(data_deco.ULSP, model='multiplicative', period=52,extrapolate_trend='freq')

# Plot
plt.rcParams.update({'figure.figsize': (16,12)})
multiplicative_decomposition.plot().suptitle('Multiplicative Decomposition', fontsize=16)
plt.tight_layout(rect=[0, 0.03, 1, 0.95])


plt.show()

from statsmodels.tsa.seasonal import STL
import matplotlib.pyplot as plt

# Decomposition de la série
decomp = STL(data_deco, period=4).fit()
seasonal = decomp.seasonal

# Composante saisonnière
plt.plot(range(len(seasonal)), seasonal)
plt.xlabel('Time')
plt.ylabel('Seasonal Component')
plt.title('Composante saionnière')
plt.show()

#Chargement des données
data_ts = pd.read_csv('oil_price.csv', index_col='Date', parse_dates=True)
data_ts.index = pd.to_datetime(data_ts.index)

data_ts

#Corrections des variations saisonnières
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Appliquer une transformation de Box-Cox pour stabiliser la variance
from scipy.stats import boxcox

# Appliquer la transformation de Box-Cox
data_boxcox, lambda_ = boxcox(data_ts.ULSP)

# Convertir la série transformée en une série pandas
data_boxcox = pd.Series(data_boxcox, index=data_ts.index)
data_diff = data_boxcox.diff().dropna()

# Appliquer une transformation de moyenne mobile pour éliminer la saisonnalité
data_seasonal = data_diff.rolling(window=4).mean().dropna()

# Afficher les graphiques de la série originale, de la série transformée et de la série corrigée
fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(10,8))
data_ts.plot(ax=ax[0])
ax[0].set_title('Série originale')
ax[0].set_xlabel('Date')
ax[0].set_ylabel('Valeur')
pd.Series(data_boxcox, index=data_ts.index).plot(ax=ax[1])
ax[1].set_title('Série transformée (Box-Cox)')
ax[1].set_xlabel('Date')
ax[1].set_ylabel('Valeur')
data_seasonal.plot(ax=ax[2])
ax[2].set_title('Série corrigée')
ax[2].set_xlabel('Date')
ax[2].set_ylabel('Valeur')
plt.tight_layout()
plt.show()

# Test d'ADF
test_adf(data_seasonal, 'Résultats du test adf')

# Découpage de la série en données d'entrainement et de test
data_prophet=pd.read_csv('oil_price.csv')
data_prophet.columns = ['ds', 'y']
data_prophet['ds']= pd.to_datetime(data_prophet['ds'])
data_prophet = data_prophet.sort_values(by='ds', ascending=True)

train, test = time_train_test_split(data_prophet, test_size=int(len(data_prophet)*0.1))

#creation de l'objet prophet
model = Prophet(yearly_seasonality='auto', weekly_seasonality='auto',
                daily_seasonality='auto',seasonality_mode='multiplicative')
#entrainement du modele
model.fit(train)
# Previsions de test
forecast = model.predict(test)

# donnees predites
print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].head())

# visualistion graphique des previsions
model.plot(forecast)

#Composantes du modèles prophet
fig2 = model.plot_components(forecast)

#definition d'une fonction pour la MAPE
def MAPE(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100
#Definition d'une fonction pour le calcul de la MAPE et la RMSE sur la ts
def evaluation_des_previsions(y_true, y_pred):

    print(f'RMSE is : {np.sqrt(metrics.mean_squared_error(y_true, y_pred))}')
    print(f'MAPE is : {MAPE(y_true, y_pred)}')

# Calcul des erreurs de prévisions
from math import *
y_trues = test['y'].values
y_preds = forecast['yhat'].values
MAE = mean_absolute_error(y_trues, y_preds)
Mape=MAPE(y_trues, y_preds)
RMSE=sqrt(mean_squared_error (y_trues, y_preds))

# Données prédites vs données réelles
plt.plot(range(103), y_trues, color='blue', label='données réelles')
plt.plot(range(103), y_preds, color='red', label='données prédites')

# Légende noms des axes
plt.legend()
plt.xlabel('x')
plt.ylabel('y')

# Show the plot
plt.show()

MAE

Mape

RMSE

#Importation des données
data_ts = pd.read_csv('oil_price.csv', index_col='Date', parse_dates=True)
data_ts.index = pd.to_datetime(data_ts.index)
#data_ts.index = pd.DatetimeIndex(data_ts.index).to_period('W')
data_ts=data_ts.ULSP

import pandas as pd
from pmdarima.arima import auto_arima


# recherche des paramètres des différentiations de SARIMA
stepwise_fit = auto_arima(data_ts, start_p=0, start_q=0, max_p=5, max_q=5,
                          seasonal=True, m=52, stepwise=True, trace=True)

# Print the optimal parameters
print(stepwise_fit.summary())

import pandas as pd
import statsmodels.api as sm

# Découpage de la série en données d'entrainement et de test
train, test = time_train_test_split(data_ts, test_size=int(len(data_ts)*0.1))

# Créer et entraîner le modèle SARIMA

from statsmodels.tsa.statespace.sarimax import SARIMAX

model = SARIMAX(train, order=(1,1,0), seasonal_order=(2,0,0,52))
sarima = model.fit()

n_test = test.shape[0]
ts_pred = sarima.forecast(steps=n_test)

# Données réelles vs données prédites
plt.plot(range(103), y_trues, color='blue', label='données réelles')
plt.plot(range(103), ts_pred, color='red', label='données prédites')

# légendes et noms des axes
plt.legend()
plt.xlabel('x')
plt.ylabel('y')

# Show the plot
plt.show()

# Erreurs des prévisions
MAE = mean_absolute_error(test, ts_pred)
Mape=MAPE(test, ts_pred)
RMSE=sqrt(mean_squared_error(test, ts_pred))

MAE

Mape

RMSE